{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../OpenNMT-py/\")\n",
    "import onmt\n",
    "import onmt.Markdown\n",
    "import torch\n",
    "import argparse\n",
    "import math\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-gpu'], dest='gpu', nargs=None, const=None, default=-1, type=<class 'int'>, choices=None, help='Device to run on', metavar=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 因为translator要用到opt，所以先全部拷贝过来了，\n",
    "\n",
    "parser = argparse.ArgumentParser(description='translate.py')\n",
    "onmt.Markdown.add_md_help_argument(parser)\n",
    "\n",
    "parser.add_argument('-model', default = \"./model/prepro_model_ppl_20.07_e13.pt\",\n",
    "                    help='Path to model .pt file')\n",
    "parser.add_argument('-src', default = \"./test_data/preprotst2016.bpe.noUndo.en\",\n",
    "                    help='Source sequence to decode (one line per sequence)')\n",
    "parser.add_argument('-src_img_dir',   default=\"\",\n",
    "                    help='Source image directory')\n",
    "parser.add_argument('-tgt', default = \"./test_data/preprotst2016.bpe.noUndo.de\",\n",
    "                    help='True target sequence (optional)')\n",
    "parser.add_argument('-output', default='pred.txt',\n",
    "                    help=\"\"\"Path to output the predictions (each line will\n",
    "                    be the decoded sequence\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "parser.add_argument('-beam_size',  type=int, default=5,\n",
    "                    help='Beam size')\n",
    "parser.add_argument('-batch_size', type=int, default=30,\n",
    "                    help='Batch size')\n",
    "parser.add_argument('-max_sent_length', type=int, default=100,\n",
    "                    help='Maximum sentence length.')\n",
    "parser.add_argument('-replace_unk', action=\"store_true\",\n",
    "                    help=\"\"\"Replace the generated UNK tokens with the source\n",
    "                    token that had highest attention weight. If phrase_table\n",
    "                    is provided, it will lookup the identified source token and\n",
    "                    give the corresponding target token. If it is not provided\n",
    "                    (or the identified source token does not exist in the\n",
    "                    table) then it will copy the source token\"\"\")\n",
    "# parser.add_argument('-phrase_table',\n",
    "#                     help=\"\"\"Path to source-target dictionary to replace UNK\n",
    "#                     tokens. See README.md for the format of this file.\"\"\")\n",
    "parser.add_argument('-verbose', action=\"store_true\",\n",
    "                    help='Print scores and predictions for each sentence')\n",
    "parser.add_argument('-dump_beam', type=str, default=\"\",\n",
    "                    help='File to dump beam information to.')\n",
    "\n",
    "parser.add_argument('-n_best', type=int, default=1,\n",
    "                    help=\"\"\"If verbose is set, will output the n_best\n",
    "                    decoded sentences\"\"\")\n",
    "parser.add_argument('-print_nbest', action='store_true',\n",
    "                    help='Output the n-best list instead of a single sentence')\n",
    "parser.add_argument('-normalize', action='store_true',\n",
    "                    help='To normalize the scores based on output length')\n",
    "parser.add_argument('-gpu', type=int, default=-1,\n",
    "                    help=\"Device to run on\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addone(f):\n",
    "    for line in f:\n",
    "        yield line\n",
    "    yield None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-md] [-model MODEL] [-src SRC]\n",
      "                             [-src_img_dir SRC_IMG_DIR] [-tgt TGT]\n",
      "                             [-output OUTPUT] [-beam_size BEAM_SIZE]\n",
      "                             [-batch_size BATCH_SIZE]\n",
      "                             [-max_sent_length MAX_SENT_LENGTH] [-replace_unk]\n",
      "                             [-verbose] [-dump_beam DUMP_BEAM]\n",
      "                             [-n_best N_BEST] [-print_nbest] [-normalize]\n",
      "                             [-gpu GPU]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/ihuangyiran/Library/Jupyter/runtime/kernel-d704f76f-18a9-44cd-921a-0762a098a590.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ihuangyiran/anaconda2/envs/py3-tf/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "opt = parser.parse_args()\n",
    "opt.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    opt = parser.parse_args()\n",
    "    opt.cuda = opt.gpu > -1\n",
    "    if opt.cuda:\n",
    "        torch.cuda.set_device(opt.gpu)\n",
    "    \n",
    "    # Always pick n_best\n",
    "    opt.n_best = opt.beam_size\n",
    "\n",
    "    \n",
    "    if opt.output == \"stdout\":\n",
    "            outF = sys.stdout\n",
    "    else:\n",
    "            outF = open(opt.output, 'w')\n",
    "\n",
    "\n",
    "    srcBatch, tgtBatch = [], []\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    tgtF = open(opt.tgt) if opt.tgt else None\n",
    "\n",
    "    if opt.dump_beam != \"\":\n",
    "        import json\n",
    "        translator.initBeamAccum()\n",
    "    \n",
    "    # here we are trying to open the file\n",
    "    inFile = None\n",
    "    if(opt.src == \"stdin\"):\n",
    "            inFile = sys.stdin\n",
    "            opt.batch_size = 1\n",
    "    else:\n",
    "      inFile = open(opt.src)\n",
    "\n",
    "    translator = onmt.Translator(opt)\n",
    "\n",
    "    for line in addone(inFile):\n",
    "        if line is not None:\n",
    "            srcTokens = line.split()\n",
    "            srcBatch += [srcTokens]\n",
    "            if tgtF:\n",
    "                tgtTokens = tgtF.readline().split() if tgtF else None\n",
    "                tgtBatch += [tgtTokens]\n",
    "\n",
    "            if len(srcBatch) < opt.batch_size:\n",
    "                continue\n",
    "        else:\n",
    "            # at the end of file, check last batch\n",
    "            if len(srcBatch) == 0:\n",
    "                break\n",
    "\n",
    "    decOut, decStates, attn = translator.translate(srcBatch, tgtBatch)\n",
    "    \n",
    "    with open(\"./test_data/hidden_value\", \"w\") as f:\n",
    "        tmp = decOut.data[-1].numpy()\n",
    "        numpy.save(f, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Pipeline_hidden(object):\n",
    "    def __init__(self, opt):\n",
    "        # 构建并读取模型\n",
    "        self.opt = opt\n",
    "        self.tt = torch.cuda if opt.cuda else torch\n",
    "        self.beam_accum = None\n",
    "\n",
    "        # 从opt.model读取模型的各种参数\n",
    "        if opt.verbose:\n",
    "                    print('Loading model from %s' % opt.model)\n",
    "        checkpoint = torch.load(opt.model,\n",
    "                               map_location=lambda storage, loc: storage)\n",
    "\n",
    "        if opt.verbose:\n",
    "                    print('Done')\n",
    "\n",
    "        # 提取词典类型\n",
    "        model_opt = checkpoint['opt']\n",
    "        self.src_dict = checkpoint['dicts']['src']\n",
    "        self.tgt_dict = checkpoint['dicts']['tgt']\n",
    "        self._type = model_opt.encoder_type \\\n",
    "            if \"encoder_type\" in model_opt else \"text\"\n",
    "\n",
    "\n",
    "        # 构建新模型框架\n",
    "        encoder = onmt.Models.Encoder(model_opt, self.src_dict)\n",
    "        decoder = onmt.Models.Decoder(model_opt, self.tgt_dict)\n",
    "\n",
    "        # 从中间层到目标词汇的映射框架\n",
    "        generator = onmt.Models.Generator(model_opt.rnn_size, self.tgt_dict)\n",
    "        model = onmt.Models.NMTModel(encoder, decoder, generator)\\\n",
    "\n",
    "        #~ for k, v in checkpoint['model'].items():\n",
    "                    #~ print k\n",
    "\n",
    "        model_state_dict = {k: v for k, v in checkpoint['model'].items()\n",
    "                                                            if 'criterion' not in k}\n",
    "\n",
    "        #~ generator = nn.Sequential(\n",
    "            #~ nn.Linear(model_opt.rnn_size, self.tgt_dict.size()),\n",
    "            #~ nn.LogSoftmax())\n",
    "\n",
    "        # 给模型参数进行赋值\n",
    "        model.load_state_dict(model_state_dict)\n",
    "        #~ generator.load_state_dict(checkpoint['generator'])\n",
    "\n",
    "        if opt.cuda:\n",
    "            model.cuda()\n",
    "            generator.cuda()\n",
    "        else:\n",
    "            model.cpu()\n",
    "            generator.cpu()\n",
    "\n",
    "\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "\n",
    "    def buildData(self, srcBatch, goldBatch):\n",
    "        # This needs to be the same as preprocess.py.\n",
    "        if self._type == \"text\":\n",
    "            srcData = [self.src_dict.convertToIdx(b,\n",
    "                                                  onmt.Constants.UNK_WORD)\n",
    "                       for b in srcBatch]\n",
    "        elif self._type == \"img\":\n",
    "            srcData = [transforms.ToTensor()(\n",
    "                Image.open(self.opt.src_img_dir + \"/\" + b[0]))\n",
    "                       for b in srcBatch]\n",
    "\n",
    "        tgtData = None\n",
    "        if goldBatch:\n",
    "            tgtData = [self.tgt_dict.convertToIdx(b,\n",
    "                       onmt.Constants.UNK_WORD,\n",
    "                       onmt.Constants.BOS_WORD,\n",
    "                       onmt.Constants.EOS_WORD) for b in goldBatch]\n",
    "\n",
    "        return onmt.Dataset(srcData, tgtData, self.opt.batch_size,\n",
    "                            self.opt.cuda, volatile=True,\n",
    "                            data_type=self._type, balance=False)\n",
    "\n",
    "    def _getBatchSize(self, batch):\n",
    "        if self._type == \"text\":\n",
    "            return batch.size(1)\n",
    "        else:\n",
    "            return batch.size(0)\n",
    "\n",
    "    def get_hidden_batch(self, srcBatch, tgtBatch):\n",
    "        # 1) run the encoder on the src\n",
    "        # 其中encoder的输入的size是：[seq_len, batch, input_size], 这里seq_len即是numWords\n",
    "        # 其中context是[seq_len, batch, hidden_size * num_directions]\n",
    "        # encStates是一个tupel，他包括:\n",
    "        #  - h_0 (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t=seq_len\n",
    "        #  - c_0 (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for t=seq_len\n",
    "        encStates, context = self.model.encoder(srcBatch)\n",
    "\n",
    "        # 这里的srcBatch原本是dataset的输出，所以应该是(src, lengths)，下面这一步取出其中src的内容。\n",
    "        srcBatch = srcBatch[0]\n",
    "        batchSize = self._getBatchSize(srcBatch)\n",
    "\n",
    "        # 获得RNN每层的节点数\n",
    "        rnnSize = context.size(2)\n",
    "\n",
    "        # 转换encState的维度为：[layers * batch * (directions * dim)]\n",
    "        encStates = (self.model._fix_enc_hidden(encStates[0]), self.model_fix_enc_hidden(encStates[1]))\n",
    "\n",
    "        decoder = self.model.decoder\n",
    "        attentionLayer = decoder.attn\n",
    "\n",
    "        # 如果类型为text，且batchSize大于1，则使用mask，原因未知？？？？？\n",
    "        # 这个mask 将被用于decoder中的Attention，目的是使得attention能够忽略掉输入句子的padding的部分内容。\n",
    "        # 但为什么仅在batchSize大于1的时候，才使用呢？？？\n",
    "        useMasking = (self._type == 'text' and batchSize > 1)\n",
    "        padMask = None\n",
    "        if useMasking:\n",
    "            padMask = srcBatch.data.eq(onmt.Constants.PAD).t() #标记pad的内容\n",
    "\n",
    "        def mask(padMask):\n",
    "            if useMasking:\n",
    "                attentionLayer.appleMask(padMask)\n",
    "\n",
    "        decStates = encStates\n",
    "        # 初始化一个decoder的输出，\n",
    "        decOut = self.model.make_init_decoder_output(context)\n",
    "        mask(padMask)\n",
    "        initOutput = self.model.make_init_decoder_output(context)\n",
    "        # decoder的输出使outputs, hidden, atten, 前两者同一般的rnn输出，atten是nn.softmax()的输出\n",
    "        # 关于tgtBatch，应该是一个size: [numWords, batchSize]的Variable(Datasetl类里面进行了转换)\n",
    "        # globalAttention的参数是input: batch x hidden_size，context: batch x seq_len x hidden_size。\n",
    "        # 对应输出的attn是batch X seq_len\n",
    "        # 所以decoder的输出output, hidden, attn的size应该分别是：\n",
    "        # [seq_len X batch X hidden_size], [num_layers X batch X hidden_size], [batch X seq_len]\n",
    "        decOut, decStates, attn = self.model.decoder(tgtBatch[:-1], decStates, context, initOutput)\n",
    "\n",
    "        return decOut, decStates, attn\n",
    "\n",
    "    def get_hidden(self, srcBatch, goldBatch):\n",
    "        # 把单词转化成对应的index，然后放进dataset中进行包装\n",
    "        dataset = self.buildData(srcBatch, goldBatch)\n",
    "        # 获得第一个Batch\n",
    "        src, tgt, indices = dataset[0]\n",
    "        batchSize = self._getBatchSize(src[0])\n",
    "\n",
    "        # 扔到translateBatch方法里面进行翻译, 这里src，tgt都是tensor类型维度为batchSize*numWord\n",
    "        decOut, decStates, attn = self.get_hidden_batch(src, tgt)\n",
    "             \n",
    "        return decOut, decStates, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.ones(2, 2), requires_grad = True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1  1\n",
       " 1  1\n",
       "[torch.FloatTensor of size 2x2]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = x.data\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
